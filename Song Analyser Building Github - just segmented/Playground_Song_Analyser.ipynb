{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54521c8",
   "metadata": {},
   "source": [
    "Getting All The Keys and HF Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2efdf427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import lyricsgenius\n",
    "import json\n",
    "# import spacy\n",
    "# import en_core_web_sm  \n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the Genius API key from the environment variables\n",
    "apikey = os.getenv('GENIUS_API')\n",
    "genius = lyricsgenius.Genius(apikey)\n",
    "hf_api = os.getenv('HF_API_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b87693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 10 songs:\n",
      "\n",
      "1. Not Afraid by Eminem (ID: 463)\n",
      "2. Not Afraid Anymore by Halsey (ID: 2953972)\n",
      "3. Be Not Afraid by John Michael Talbot (ID: 1278257)\n",
      "4. I'm Not Afraid of Anything by Andréa Burns (ID: 1149315)\n",
      "5. She's Not Afraid by One Direction (ID: 187985)\n",
      "6. Not Afraid by Jesus Culture (Ft. Kim Walker-Smith) (ID: 3835340)\n",
      "7. я не боюсь ошибаться (i’m not afraid to make mistakes) by ​mzlff & Серега Пират (Serega Pirat) (ID: 8950851)\n",
      "8. Not Afraid (Live) by Red Rocks Worship (ID: 3252994)\n",
      "9. Not Afraid of Dying by Josh A & iamjakehill (ID: 4012143)\n",
      "10. We Are Not Afraid (Remix) by 2Pac (Ft. The Notorious B.I.G.) (ID: 515419)\n",
      "You selected the song with ID: 463\n",
      "Not Afraid by Eminem\n",
      "Searching for \"Not Afraid\" by Eminem...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the Genius API key from the environment variables\n",
    "apikey = os.getenv('GENIUS_API')\n",
    "import requests\n",
    "import lyricsgenius\n",
    "import json\n",
    "genius = lyricsgenius.Genius(apikey)\n",
    "\n",
    "def fetch_song_details(song_id):\n",
    "    \"\"\"Fetch details of a specific song using its ID.\"\"\"\n",
    "    base_url = \"https://api.genius.com\"\n",
    "    song_url = f\"{base_url}/songs/{song_id}\"\n",
    "    headers = {'Authorization': f'Bearer {apikey}'}\n",
    "    \n",
    "    response = requests.get(song_url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching song details: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "    return response.json()['response']['song']\n",
    "\n",
    "# Search for a song based on user input\n",
    "search_query = input(\"Enter Song Name: \")\n",
    "results = genius.search(search_query)\n",
    "\n",
    "# Convert results to JSON string (if needed)\n",
    "json_string = json.dumps(results)\n",
    "\n",
    "# Extract the hits from the results\n",
    "hits = results['hits']\n",
    "\n",
    "# Display song titles in enumerated form\n",
    "if hits:\n",
    "    print(f\"\\nFound {len(hits)} songs:\\n\")\n",
    "    for i, hit in enumerate(hits):\n",
    "        song_title = hit['result']['full_title']\n",
    "        song_id = hit['result']['id']\n",
    "        print(f\"{i + 1}. {song_title} (ID: {song_id})\")  \n",
    "#     # Get user selection\n",
    "    while True:\n",
    "        try:\n",
    "            song_choice = int(input(\"\\nSelect a song by entering its number: \")) - 1\n",
    "            if 0 <= song_choice < len(hits):\n",
    "                selected_song_id = hits[song_choice]['result']['id']\n",
    "                print(f\"You selected the song with ID: {selected_song_id}\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid selection. Please try again.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "    # Fetch song details using the selected song ID\n",
    "    song_details = fetch_song_details(selected_song_id)\n",
    "    artist_name = song_details['primary_artist']['name']\n",
    "    artist_song = song_details['title']\n",
    "    print(f\"{artist_song} by {artist_name}\")\n",
    "    lyrics = genius.search_song(artist_song,artist_name)\n",
    "    \n",
    "#     if song_details:\n",
    "#         # Display song details\n",
    "#         print(f\"\\nDetails for '{song_details['title']}':\")\n",
    "#         print(f\"Artist: {song_details['primary_artist']['name']}\")\n",
    "#         print(f\"Release Date: {song_details.get('release_date', 'N/A')}\")\n",
    "#         print(f\"Lyrics State: {song_details['lyrics_state']}\")\n",
    "#         print(f\"Lyrics URL: {song_details['url']}\")\n",
    "#         print(f\"Text Format: {song_details.get('text_format', 'N/A')}\")\n",
    "#         # If you want to print the lyrics, you can fetch them separately if needed\n",
    "\n",
    "# else:\n",
    "#     print(f\"No songs found for the search query '{search_query}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c7c1fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.26.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sidhe\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (2024.7.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\Users\\sidhe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import notebook_login\n",
    "from huggingface_hub import login\n",
    "login(token=hf_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "640c78b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '66f2f8c96493ca67946f1b67', 'name': 'sidheshtavare', 'fullname': 'Sidhesh Mahesh Tavare', 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': '/avatars/87dd7576931e0dccda3801f8129d6d29.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'Hello', 'role': 'fineGrained', 'createdAt': '2024-10-18T17:52:36.376Z', 'fineGrained': {'canReadGatedRepos': True, 'global': ['inference.serverless.write', 'discussion.write', 'post.write'], 'scoped': [{'entity': {'_id': '66f2f8c96493ca67946f1b67', 'type': 'user', 'name': 'sidheshtavare'}, 'permissions': ['repo.content.read', 'repo.write', 'inference.endpoints.infer.write', 'inference.endpoints.write', 'user.webhooks.read', 'user.webhooks.write', 'collection.read', 'collection.write', 'discussion.write', 'user.billing.read']}]}}}}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import whoami\n",
    "\n",
    "# Check your login status\n",
    "user_info = whoami()\n",
    "print(user_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc2b0d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 16.25s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the text generation pipeline with the gemma model and set to GPU\n",
    "generator = pipeline(\"text-generation\", model=\"google/gemma-2-2b-it\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7d78153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Requirement already satisfied: transformers in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.46.1)\n",
      "Requirement already satisfied: torch in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.5.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (73.0.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sidhe\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.7.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if GPU is available\n",
    "!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43d8fa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:33<00:00, 46.93s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.20 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 9.82 GiB is allocated by PyTorch, and 53.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/gemma-2-2b-it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[0;32m      4\u001b[0m input_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnce upon a time in a futuristic world\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sidhe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1164\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1162\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m processor\n\u001b[1;32m-> 1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sidhe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:96\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[0;32m     98\u001b[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[0;32m     99\u001b[0m     )\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params:\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sidhe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:935\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[1;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    934\u001b[0m ):\n\u001b[1;32m--> 935\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;66;03m# If the model can generate, create a local generation config. This is done to avoid side-effects on the model\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;66;03m# as we apply local tweaks to the generation config.\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcan_generate():\n",
      "File \u001b[1;32mc:\\Users\\sidhe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:3157\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   3153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3155\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3156\u001b[0m         )\n\u001b[1;32m-> 3157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sidhe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sidhe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sidhe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sidhe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sidhe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.20 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 9.82 GiB is allocated by PyTorch, and 53.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"google/gemma-2-2b-it\", device=0)\n",
    "\n",
    "# Generate text\n",
    "input_prompt = \"Once upon a time in a futuristic world\"\n",
    "output = generator(input_prompt, max_length=50, num_return_sequences=1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4815b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(To take a stand, to take a stand) It's been a ride\n",
      "(Everybody, everybody) I guess I had to\n",
      "(Come take my hand, come take my hand) Go to that place to get to this one\n",
      "(We'll walk this road together, through the storm) Now some of you\n",
      "(Whatever weather, cold or warm) Might still be in that place\n",
      "(Just lettin' you know that you're not alone) If you're tryna get out\n",
      "(Holla if you feel like you've been down the same road) Just follow me, I'll get you there\n",
      "\n",
      "[Verse 1]\n",
      "You can try and read my lyrics off of this paper before I lay 'em\n",
      "But you won't take the sting out these words before I say 'em\n",
      "'Cause ain't no way I'ma let you stop me from causin' mayhem\n",
      "When I say I'ma do somethin', I do it\n",
      "I don't give a damn what you think, I'm doin' this for me\n",
      "So fuck the world, feed it beans, it's gassed up if it thinks it's stoppin' me\n",
      "I'ma be what I set out to be, without a doubt, undoubtedly\n",
      "And all those who look down on me, I'm tearin' down your balcony\n",
      "No ifs, ands or buts, don't try to ask him why or how can he\n",
      "From Infinite down to the last Relapse album\n",
      "He's still shittin' whether he's on salary, paid hourly\n",
      "Until he bows out or he shits his bowels out of him\n",
      "Whichever comes first, for better or worse\n",
      "He's married to the game like a \"Fuck you\" for Christmas\n",
      "His gift is a curse, forget the Earth, he's got the urge to pull his dick from the dirt\n",
      "And fuck the whole universe\n",
      "See Eminem LiveGet tickets as low as $84You might also like[Chorus]\n",
      "I'm not afraid, I'm not afraid\n",
      "To take a stand, to take a stand\n",
      "Everybody, everybody\n",
      "Come take my hand, come take my hand\n",
      "We'll walk this road together, through the storm\n",
      "Whatever weather, cold or warm\n",
      "Just lettin' you know that you're not alone\n",
      "Holla if you feel like you've been down the same road\n",
      "\n",
      "[Verse 2]\n",
      "Okay, quit playin' with the scissors and shit, and cut the crap\n",
      "I shouldn't have to rhyme these words in a rhythm for you to know it's a wrap\n",
      "You said you was king, you lied through your teeth\n",
      "For that, fuck your feelings, instead of gettin' crowned, you're gettin' capped\n",
      "And to the fans, I'll never let you down again, I'm back\n",
      "I promise to never go back on that promise\n",
      "In fact, let's be honest, that last Relapse CD was \"ehh\"\n",
      "Perhaps I ran them accents into the ground\n",
      "Relax, I ain't goin' back to that now\n",
      "All I'm tryna say is get back, click-clack-blaow\n",
      "'Cause I ain't playin' around\n",
      "There's a game called circle and I don't know how\n",
      "I'm way too up to back down\n",
      "But I think I'm still tryna figure this crap out\n",
      "Thought I had it mapped out, but I guess I didn't\n",
      "This fuckin' black cloud still follows me around\n",
      "But it's time to exorcise these demons\n",
      "These motherfuckers are doin' jumpin' jacks now\n",
      "[Chorus]\n",
      "I'm not afraid, I'm not afraid\n",
      "To take a stand, to take a stand\n",
      "Everybody, everybody\n",
      "Come take my hand, come take my hand\n",
      "We'll walk this road together, through the storm\n",
      "Whatever weather, cold or warm\n",
      "Just lettin' you know that you're not alone\n",
      "Holla if you feel like you've been down the same road (Same road, same road)\n",
      "\n",
      "[Bridge]\n",
      "And I just can't keep livin' this way\n",
      "So startin' today\n",
      "I'm breakin' out of this cage\n",
      "I'm standin' up, I'ma face my demons\n",
      "I'm mannin' up, I'ma hold my ground\n",
      "I've had enough, now I'm so fed up\n",
      "Time to put my life back together right now\n",
      "\n",
      "[Verse 3]\n",
      "It was my decision to get clean, I did it for me\n",
      "Admittedly I probably did it subliminally for you\n",
      "So I could come back a brand-new me, you helped see me through\n",
      "And don't even realize what you did, believe me, you\n",
      "I've been through the wringer\n",
      "But they can do little to the middle finger\n",
      "I think I got a tear in my eye, I feel like the king of\n",
      "My world, haters can make like bees with no stingers\n",
      "And drop dead: no more beef lingers\n",
      "No more drama from now on\n",
      "I promise to focus solely on handlin' my responsibilities as a father\n",
      "So I solemnly swear to always treat this roof like my daughters and raise it\n",
      "You couldn't lift a single shingle on it\n",
      "'Cause the way I feel, I'm strong enough to go to the club\n",
      "Or the corner pub and lift the whole liquor counter up\n",
      "'Cause I'm raisin' the bar, I'd shoot for the moon\n",
      "But I'm too busy gazin' at stars, I feel amazin', and\n",
      "[Chorus]\n",
      "I'm not afraid, I'm not afraid\n",
      "To take a stand, to take a stand\n",
      "Everybody, everybody\n",
      "Come take my hand, come take my hand\n",
      "We'll walk this road together, through the storm\n",
      "Whatever weather, cold or warm\n",
      "Just lettin' you know that you're not alone\n",
      "Holla if you feel like you've been down the same road (Same road, same-)431Embed\n"
     ]
    }
   ],
   "source": [
    "raw_lyrics = lyrics.lyrics\n",
    "raw_lyrics = raw_lyrics.splitlines()\n",
    "raw_lyrics= '\\n'.join(raw_lyrics[2:])\n",
    "print(raw_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0339c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction with song lyrics input\n",
    "instruction = \"\"\"Analyze the following song lyrics for appropriateness based on language and themes. Label the song as either EXPLICIT or CLEAN. A song is considered EXPLICIT if it contains content that is offensive or unsuitable for children, such as strong language, including curse words; references to violence, physical or mental abuse; references to sexual behavior; discriminatory language; or sexual, violent, or offensive imagery or sounds. If the song is labeled EXPLICIT, provide a brief summary of the phrases or words that contribute to this rating, highlighting the negative aspects of the song. If the lyrics are free from all explicit content, mark the song as CLEAN and provide a short summary of any positive or uplifting themes the song conveys. Focus on the overall tone and primary themes rather than analyzing line by line.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab706763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for: 0:03:45\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Define the start time\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Display elapsed time while running\n",
    "try:\n",
    "    # Running in a loop to update the elapsed time\n",
    "    while True:\n",
    "        # Calculate the elapsed time\n",
    "        elapsed_time = datetime.datetime.now() - start_time\n",
    "        # Format the time string (hours:minutes:seconds)\n",
    "        elapsed_str = str(elapsed_time).split(\".\")[0]\n",
    "        \n",
    "        # Display the current elapsed time\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Running for: {elapsed_str}\")\n",
    "        \n",
    "        # Short delay to update every second\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Break if response generation finishes\n",
    "        if 'response' in locals():\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "response = generator(\n",
    "    instruction + \"\\n\\n\" + raw_lyrics,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=1024,         # Reasonable length for output\n",
    "    min_length=100,             # Ensures a detailed response\n",
    "    temperature=0.8,            # Balances creativity\n",
    "    top_k=50,                   # Limits sampling to top-k tokens\n",
    "    top_p=0.9,                  # Uses nucleus sampling\n",
    "    repetition_penalty=1.3,      # Discourages repetition\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Extract just the relevant output\n",
    "output_text = response[0]['generated_text']\n",
    "print(\"\\nCompleted in:\", str(datetime.datetime.now() - start_time).split(\".\")[0])\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
